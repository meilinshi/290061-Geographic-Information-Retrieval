{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059bf067",
   "metadata": {},
   "source": [
    "# Lab 9 - Geospatial Semantics II\n",
    "\n",
    "Th 28.11.2024 15:00-17:00\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b8b9f",
   "metadata": {},
   "source": [
    "## 0. (Optional) Retrieving Data from OSM with Pyrosm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694d66d",
   "metadata": {},
   "source": [
    "![pyrosm](../figs/lab9_figs/pyrosm.png)\n",
    "\n",
    "#### Retrieving Data\n",
    "`from pyrosm import OSM, get_data`\n",
    "\n",
    "`vienna = get_data(\"wien\")` # Download data for Vienna\n",
    "\n",
    "`osm = OSM(vienna)` # Initialize the reader object for Vienna\n",
    "\n",
    "`vienna_poi = osm.get_pois()` # Extract the POIs in Vienna from OSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1e22c",
   "metadata": {},
   "source": [
    "In the first command, we downloaded the data for “wien” using the get_data function. This function in fact automates the data downloading process and stores the data locally in a temporary folder. The next step was to initialize a reader object called osm. The OSM() function takes the filepath to a given osm.pbf file as an input. Notice that at this point we actually didn’t yet read any data into DataFrame.\n",
    "\n",
    "OSM contains a lot of information about the world, which is contributed by citizens like you and me. In principle, we can retrieve information under various themes from OSM using the following functions.\n",
    "- road networks –> osm.get_network()\n",
    "- buildings –> osm.get_buildings()\n",
    "- Points of Interest (POI) –> osm.get_pois()\n",
    "- landuse –> osm.get_landuse()\n",
    "- natural elements –> osm.get_natural()\n",
    "- boundaries –> osm.get_boundaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3014bc15",
   "metadata": {},
   "source": [
    "## 1. Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf15385",
   "metadata": {},
   "source": [
    "The dataset we downloaded from moodle is preprocessed from the results of `osm.get_pois()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('vienna_poi.csv')\n",
    "\n",
    "## removes rows from if the 'amenity' column contains missing values\n",
    "df.dropna(subset=['amenity'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de3599a",
   "metadata": {},
   "source": [
    "## 2. Aggregate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by postcode and aggregate values in the `amenity` column into a list\n",
    "\n",
    "df_group = df.groupby('postcode')['amenity'].agg(list).reset_index()\n",
    "df_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbf038",
   "metadata": {},
   "source": [
    "After aggregating the data, we need to prepare a document list (called corpus here) as the input to the model. This corpus is a list of list, where the inner list is a place type list composing the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb07e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(df_group['amenity'])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5bff1c",
   "metadata": {},
   "source": [
    "## 3. Word2Vec\n",
    "With such a corpus, we then can train the neural network to learn the embedding for words in the corpus based on their use in the document. All the detailed processes (e.g., initializing, sampling, optimization, etc.) are encapsulated in the function called Word2Vec provided by gensim’s models module (it also includes other advanced NLP models such as ldamodel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdbe770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "## train the model\n",
    "model = Word2Vec(corpus, min_count=1, vector_size=100, workers=3, window =3, sg = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc16712a",
   "metadata": {},
   "source": [
    "For the full documentation of Word2Vec, please check it official webpage: https://radimrehurek.com/gensim/models/word2vec.html. In this example, you will see in the parameters corpus, the input:\n",
    "- **min_count**, if the frequency of a word is less than it, it will not be considered in the model; \n",
    "- **vector_size**, dimensionality of the word embedding vector; \n",
    "- **workers**, the number of worker threads to train the model (it depends on your systems capability); \n",
    "- **window**, maximum distance between the current and predicted word (define the length of the context);\n",
    "- **sg**, training algorithm: 1 for skip-gram; 0 for CBOW (Continuous Bag of Words).\n",
    "\n",
    "**How to determine these parameters sometimes depends on modeler’s experience as well as the experiment results. In general Machine Learning (ML)/Deep Learning (DL) field, scientists usually conduct a lot of experienment in order to tune the model to its best performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## with the trained model, we can see the word embedding of any word (place type here)\n",
    "## wv is short for word vector\n",
    "\n",
    "model.wv['restaurant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80293dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the (semantic) simiarity between words (place types)\n",
    "\n",
    "sim_restaurant_cafe = model.wv.similarity('restaurant', 'cafe')\n",
    "sim_restaurant_cafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c9b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cafe_bank = model.wv.similarity('cafe', 'bank')\n",
    "sim_cafe_bank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376aaa2",
   "metadata": {},
   "source": [
    "We can also use the built-in function most_similar() to get the most similar word to the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d903de",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('restaurant')[:5]\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ae0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('doctors')[:5]\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ac39b",
   "metadata": {},
   "source": [
    "## 4. Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea686c4",
   "metadata": {},
   "source": [
    "Similar to Word2Vec, we can also compute the document embedding with [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede0239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to TaggedDocument format\n",
    "tagged_data = []\n",
    "for idx, row in df_group.iterrows():\n",
    "    words = row['amenity']\n",
    "    tags = row['postcode']\n",
    "    tagged_data.append(TaggedDocument(words=words, tags=[str(tags)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace301cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(min_count=1, vector_size=100, window=3, workers=3)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8300017",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = model.dv.similarity('1010', '1070')\n",
    "similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = model.dv.similarity('1010', '1190')\n",
    "similarity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74e703",
   "metadata": {},
   "source": [
    "## 5. Submission\n",
    "\n",
    "Feel free to adjust the parameters, explore Word2Vec and Doc2Vec with different place types and districts. \n",
    "\n",
    "Write a few sentences whether the results make sense to you: \n",
    "\n",
    "...\n",
    "\n",
    "and submit the .ipynb file on Moodle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}